# Playwright Web Application Testing Workflow
# This workflow is designed for testing web applications with Playwright
# It includes features like sharding, scheduled regression tests, and detailed reporting
# For CLI testing, use playwright-cli-tests.yml instead

name: Playwright Web Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  schedule:
    - cron: '0 0 * * *'  # Daily regression test
  workflow_dispatch:  # Allow manual triggering

jobs:
  # Detect deployment URL (Vercel or custom)
  detect-deployment:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    outputs:
      url: ${{ steps.detect.outputs.url }}
      detected: ${{ steps.detect.outputs.detected }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'
      
      - name: Check for manual deployment URL
        id: manual
        run: |
          if [ -n "${{ secrets.DEPLOYMENT_URL }}" ]; then
            echo "url=${{ secrets.DEPLOYMENT_URL }}" >> $GITHUB_OUTPUT
            echo "detected=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Using manual deployment URL from secrets"
          elif [ -n "${{ secrets.VISUAL_TEST_URL }}" ]; then
            echo "url=${{ secrets.VISUAL_TEST_URL }}" >> $GITHUB_OUTPUT
            echo "detected=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Using visual test URL from secrets"
          else
            echo "detected=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è No manual URL configured, will attempt auto-detection"
          fi
      
      - name: Detect Vercel Preview URL
        if: steps.manual.outputs.detected != 'true'
        id: vercel
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          VERCEL_TOKEN: ${{ secrets.VERCEL_TOKEN }}
        run: |
          # Try to detect Vercel preview URL
          echo "üîç Attempting to detect Vercel preview URL..."
          
          # Check if vercel-preview utility exists
          if [ -f "cli/utils/vercel-preview.js" ]; then
            node cli/utils/vercel-preview.js auto --debug || true
          else
            # Fallback to GitHub API check
            PR_NUMBER=${{ github.event.pull_request.number }}
            OWNER=${{ github.repository_owner }}
            REPO=${{ github.event.repository.name }}
            
            # Check PR comments for Vercel bot
            VERCEL_URL=$(gh api repos/${OWNER}/${REPO}/issues/${PR_NUMBER}/comments \
              --jq '.[] | select(.user.login == "vercel[bot]" or .body | contains("vercel.app")) | .body' \
              | grep -oE 'https://[a-zA-Z0-9.-]+\.vercel\.app' | head -1 || true)
            
            if [ -n "$VERCEL_URL" ]; then
              echo "url=$VERCEL_URL" >> $GITHUB_OUTPUT
              echo "detected=true" >> $GITHUB_OUTPUT
              echo "‚úÖ Found Vercel preview URL: $VERCEL_URL"
            else
              echo "detected=false" >> $GITHUB_OUTPUT
              echo "‚ö†Ô∏è No Vercel preview URL found"
            fi
          fi
      
      - name: Set deployment URL output
        id: detect
        run: |
          if [ "${{ steps.manual.outputs.detected }}" == "true" ]; then
            echo "url=${{ steps.manual.outputs.url }}" >> $GITHUB_OUTPUT
            echo "detected=true" >> $GITHUB_OUTPUT
          elif [ "${{ steps.vercel.outputs.detected }}" == "true" ]; then
            echo "url=${{ steps.vercel.outputs.url }}" >> $GITHUB_OUTPUT
            echo "detected=true" >> $GITHUB_OUTPUT
          else
            echo "url=${{ secrets.BASE_URL || 'http://localhost:3000' }}" >> $GITHUB_OUTPUT
            echo "detected=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è Using fallback URL: ${{ secrets.BASE_URL || 'http://localhost:3000' }}"
          fi
      
      - name: Wait for deployment to be ready
        if: steps.detect.outputs.detected == 'true'
        run: |
          URL="${{ steps.detect.outputs.url }}"
          echo "‚è≥ Waiting for deployment to be ready: $URL"
          
          MAX_ATTEMPTS=60
          ATTEMPT=0
          
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            if curl -s -o /dev/null -w "%{http_code}" "$URL" | grep -q "^[23]"; then
              echo "‚úÖ Deployment is ready!"
              exit 0
            fi
            
            ATTEMPT=$((ATTEMPT + 1))
            echo "Attempt $ATTEMPT/$MAX_ATTEMPTS - Deployment not ready yet, waiting..."
            sleep 5
          done
          
          echo "‚ö†Ô∏è Deployment did not become ready in time, proceeding anyway"

  test:
    timeout-minutes: 60
    runs-on: ubuntu-latest
    needs: [detect-deployment]
    if: always()
    strategy:
      fail-fast: false
      matrix:
        shardIndex: [1, 2, 3, 4]
        shardTotal: [4]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Install Playwright Browsers
        run: npx playwright install --with-deps
      
      - name: Run Playwright tests
        run: npx playwright test --shard=${{ matrix.shardIndex }}/${{ matrix.shardTotal }}
        env:
          BASE_URL: ${{ needs.detect-deployment.outputs.url || secrets.BASE_URL || 'http://localhost:3000' }}
          DEPLOYMENT_URL: ${{ needs.detect-deployment.outputs.url }}
          IS_VERCEL_PREVIEW: ${{ needs.detect-deployment.outputs.detected }}
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report-${{ matrix.shardIndex }}
          path: playwright-report/
          retention-days: 30
      
      - name: Upload screenshots on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-screenshots-${{ matrix.shardIndex }}
          path: .playwright/screenshots/
      
      - name: Upload videos on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-videos-${{ matrix.shardIndex }}
          path: test-results/
      
      - name: Update Memory System
        if: always()
        run: |
          # Create test results directory if it doesn't exist
          mkdir -p .ai/memory/test-results
          
          # Parse test results if they exist
          if [ -f "test-results.json" ] || [ -f "playwright-report/results.json" ]; then
            node -e "
            const fs = require('fs');
            const resultsFile = fs.existsSync('test-results.json') ? 'test-results.json' : 'playwright-report/results.json';
            const results = JSON.parse(fs.readFileSync(resultsFile, 'utf8'));
            const date = new Date().toISOString();
            
            // Create test result memory with metadata
            const memory = {
              date: date,
              source: 'github-action',
              created_by: 'ci',
              shard: '${{ matrix.shardIndex }}/${{ matrix.shardTotal }}',
              passed: results.stats?.expected || 0,
              failed: results.stats?.unexpected || 0,
              flaky: results.stats?.flaky || 0,
              skipped: results.stats?.skipped || 0,
              duration: results.duration || 0
            };
            
            // Only save if tests failed or had issues
            if (memory.failed > 0 || memory.flaky > 0) {
              fs.writeFileSync(
                '.ai/memory/test-results/failure-shard-${{ matrix.shardIndex }}-' + date.replace(/:/g, '-') + '.json',
                JSON.stringify(memory, null, 2)
              );
              console.log('Test failures documented');
            } else {
              console.log('All tests passed, skipping memory update');
            }
            
            // Update failures directory if there are failures
            if (memory.failed > 0) {
              fs.mkdirSync('.ai/memory/test-results/failures', { recursive: true });
              const failureContent = [
                '---',
                'source: github-action',
                'created_by: ci',
                'created_at: ' + date,
                'shard: ${{ matrix.shardIndex }}/${{ matrix.shardTotal }}',
                '---',
                '',
                '# Test Failures Report',
                '',
                '- Failed: ' + memory.failed,
                '- Passed: ' + memory.passed,
                '- Flaky: ' + memory.flaky,
                '- Duration: ' + memory.duration + 'ms',
                '',
                '## Failed Tests',
                results.failures?.map(f => '- ' + f.title + ': ' + f.error).join('\\n') || 'No details available'
              ].join('\\n');
              fs.writeFileSync(
                '.ai/memory/test-results/failures/' + date.replace(/:/g, '-') + '.md',
                failureContent
              );
            }
            "
          fi

  merge-reports:
    if: always()
    needs: [test]
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 'lts/*'
      
      - name: Download all reports
        uses: actions/download-artifact@v4
        with:
          path: all-reports/
      
      - name: Merge reports
        run: |
          npx playwright merge-reports --reporter html ./all-reports || true
      
      - name: Upload merged report
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report-complete
          path: playwright-report/
          retention-days: 30
      
      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            // Collect all shard results
            let totalPassed = 0, totalFailed = 0, totalFlaky = 0, totalSkipped = 0;
            const shardResults = [];
            
            for (let i = 1; i <= 4; i++) {
              const shardFile = `all-reports/playwright-report-${i}/results.json`;
              if (fs.existsSync(shardFile)) {
                const results = JSON.parse(fs.readFileSync(shardFile, 'utf8'));
                totalPassed += results.stats?.expected || 0;
                totalFailed += results.stats?.unexpected || 0;
                totalFlaky += results.stats?.flaky || 0;
                totalSkipped += results.stats?.skipped || 0;
                shardResults.push(results);
              }
            }
            
            const status = totalFailed === 0 ? '‚úÖ All tests passed!' : '‚ùå Some tests failed';
            
            const comment = `## üé≠ Playwright Test Results
            
            ${status}
            
            | Status | Count |
            |--------|-------|
            | ‚úÖ Passed | ${totalPassed} |
            | ‚ùå Failed | ${totalFailed} |
            | ‚ö†Ô∏è Flaky | ${totalFlaky} |
            | ‚è≠Ô∏è Skipped | ${totalSkipped} |
            
            **Test Execution:** Sharded across 4 parallel jobs
            **Duration:** ~${Math.max(...shardResults.map(r => r.duration || 0)) / 1000}s
            
            [üìä View Full Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            
            ${totalFailed > 0 ? '### Failed Tests\nPlease check the artifacts for screenshots and videos of failed tests.' : ''}
            
            <details>
            <summary>Test Configuration</summary>
            
            - Browsers: Chromium, Firefox, WebKit
            - Parallel Shards: 4
            - Retry Attempts: 2
            - Base URL: ${process.env.BASE_URL || 'http://localhost:3000'}
            
            </details>`;
            
            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            const botComment = comments.find(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('üé≠ Playwright Test Results')
            );
            
            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            }
      
      - name: Create Visual Regression Report
        if: always()
        run: |
          # Check for visual differences
          if ls .playwright/test-results/**/diff-*.png 1> /dev/null 2>&1; then
            echo "Visual regression differences detected!"
            
            # Create markdown report
            cat > visual-regression-report.md << EOF
          # Visual Regression Report
          
          Visual differences were detected in the following tests:
          
          EOF
            
            for diff in .playwright/test-results/**/diff-*.png; do
              echo "- $(basename $diff)" >> visual-regression-report.md
            done
            
            echo "" >> visual-regression-report.md
            echo "Please review the visual differences in the artifacts." >> visual-regression-report.md
          fi
      
      - name: Check test status
        if: always()
        run: |
          # Exit with error if tests failed
          if [ -f ".ai/memory/test-results/latest.json" ]; then
            failed=$(node -e "
              const results = JSON.parse(require('fs').readFileSync('.ai/memory/test-results/latest.json', 'utf8'));
              console.log(results.stats?.unexpected || 0);
            ")
            
            if [ "$failed" -gt 0 ]; then
              echo "‚ùå $failed tests failed"
              exit 1
            else
              echo "‚úÖ All tests passed"
            fi
          fi